<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Xiang Zhang</title>
    <link>http://xzh.me/categories/deep-learning/</link>
    <description>Recent content in Deep Learning on Xiang Zhang</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 23 Jan 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://xzh.me/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Evolve to Sum</title>
      <link>http://xzh.me/posts/evolvetosum/</link>
      <pubDate>Sat, 23 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://xzh.me/posts/evolvetosum/</guid>
      <description>

&lt;p&gt;Here is one simple question: how to make machines learn to sum up two numbers? Of course, this problem largely depends on how the numbers are represented. If they are represented in any of the finite-precision float-point format, a simple regression where both weights are one would solve the problem. But that&amp;rsquo;s not what I mean here. What I mean is, given the symbolic representation of numbers (i.e., each number is a sequence of digits), how could a machine learn to sum them up?&lt;/p&gt;

&lt;p&gt;There have been research projects in this realm, but the results are far from satisfactory and none of them can produce a machine that can sum up two numbers of arbitrary length. In this blog I want to offer an alternative view of this problem by dichotomizing learning into individual learning and evolutionary learning. Then, I provide an initial thought on the possibility of a new learning paradigm that is inspired not only from how each of us learns, but also how knowledge is formed during the entire human evolutionary process.&lt;/p&gt;

&lt;h3 id=&#34;learning-to-sum:f4c76188421e5843dda1963eace05e1c&#34;&gt;Learning to Sum&lt;/h3&gt;

&lt;p&gt;Yesterday I was invited to &lt;a href=&#34;https://www.zhihu.com/question/39727411/answer/82778981&#34;&gt;answer a question on Zhihu&lt;/a&gt;, a Chinese equivalent of Quora, about whether deep learning models can be used to learn to sum. Immediately I thought that the question is actually more difficult to answer than it appears to be, partly because it is always easier to disprove the possibility of something than to prove it.&lt;/p&gt;

&lt;p&gt;Knowing that we have very limited results in learning arithmetic operations, I started to recall a thought inspired by the book &amp;ldquo;Probably Approximately Correct: Natureâ€™s Algorithms for Learning and Prospering in a Complex World&amp;rdquo;, written by Turing award laureate Leslie G. Valiant. The book is mainly about an analogy of the probably approximately correct (PAC) learning theory and the evolutionary process of human beings. But my thought is only tangentially related to this analogy. It is more about a limitation on the current connectionist machine learning paradigm in relation to the dichotomy of individual learning and evolutionay process.&lt;/p&gt;

&lt;p&gt;It is better explained using the problem in question &amp;ndash; summation. The way each of us learns arithmetic operations such as summation is not quite what connectionist machine learning paradigm advocates &amp;ndash; end-to-end learning by showing result examples of two numbers summing together. Rather, we learn arithmetic operations via a strongly supervised process with the help of our math teachers in elementary school. When learning to sum, each of us were not only presented with examples of correct or incorrect sums, but also the proceeding steps of this arithmetic operation. If we were to train a deep learning model to do summation in this way, we would start with perhaps a simple recurrent neural net, design a process for this network to do summation, and train the network using samples of this process in a strongly supervised fashion. We can think of ourselves as the math teachers of this student recurrent neural network, teaching it not only right from wrong but also the steps to make it right.&lt;/p&gt;

&lt;p&gt;What is missing in the learning process above is the question of where summation comes from, or even where the concept of numbers comes from. We can find inspiration of such questions in humans, but it would no longer be possible to think of it in the scope of each human individual. Rather, we would have to think of human race as a whole and find answers in its entire evolutionary process. The answer of such questions immediately becomes demystified in this way of thinking, in that the concept of numbers and the process of arithmetic comes from generations of historical human activities. First of all, there is necessity of counting because we have learnt to produce and store goods. That was the reason for the concept of numbers. Then, when human society is developed enough for each individual to produce extra goods than for himself, there comes trading. Arithmetic naturally becomes a necessity, and since then we started to dissemminate the correct way of doing arithmetic until there is education and we started to teach it generation by generation. The description here is an extremely condensed version, and a better storytellling can be found in &lt;a href=&#34;http://www.bbc.co.uk/programmes/b00dwf4f&#34;&gt;BBC&amp;rsquo;s documentary &amp;lsquo;The Story of Maths&amp;rsquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The discussion above also hints on one problem of current generation deep learning research. The learning paradigm we used to design models is unexceptionally individual learning. One may argue that there is some aspect of deep learning models which implements some of evolution&amp;rsquo;s achievements &amp;ndash; such as convolutional network implements an analogy of our visual cortex (not the eye &amp;ndash; the eye is analogized by a camera). But this is only a slighly blurred line between individual learning and evolutionary learning. It is apparently impossible for evolution to be responsible for every part of the visual system &amp;ndash; maybe some lower layers that detect edges, but definitely not the upper layers where we form concepts and classification of objects. It does not disprove the necessity of a evolutionary learning paradigm.&lt;/p&gt;

&lt;h3 id=&#34;evolve-to-sum:f4c76188421e5843dda1963eace05e1c&#34;&gt;Evolve to Sum&lt;/h3&gt;

&lt;p&gt;Summation and other arithmetic operations are not the only things that evolutionary learning models. Common sense, culture, art and language &amp;ndash; traditionally hard to model by connectionist ideas &amp;ndash; could also be thought of as by-products of human evolution. If we have a way to let machines to evolve themselves, it may generate its own common sense, culture and art. This is not at all what evolutionary algorithm is about &amp;ndash; we are talking about evolution of models in a more literal sense.&lt;/p&gt;

&lt;p&gt;Admittedly, there are many problems that are unclear about an evolutionary learning process. The first one is whether there is an objective for machine evolution. I personally think that this is more about what the models should do rather than what is the optimization objective. Like human beings, the goal of a machine should be about survival in the world it exists. This suggests a different design principle for machines &amp;ndash; rather than to design machines to assist our daily lives, we would design machines that strive to surve themselves. It means we must equip machines with the ability of control its own survival needs. In the context of our current generation eletronic technology, one way to do this is probably equip it with the ability to plug itself into power outlets, at least mechanically. Then, we can use our current machine learning methods to let it learn how to get plugged with power in its software.&lt;/p&gt;

&lt;p&gt;The second problem is how could machines reproduce, since only with reproduction we would have generations of machines to constitute the process of evolution. One cheap way to do this is probably take over reproduction ourselves. We can keep producing copies of such machines, and evolution becomes choosing the best surviving learning machines to initialize the new ones. When technology is advanced enough, there will probably be a time that one of the machine will learn to reproduce by themselves, but we probably would not want to give the machines such ability.&lt;/p&gt;

&lt;p&gt;The last problem is what the software of such machines should look like. I believe it should consist of three parts: perception, reasoning and action. The perception part could be on vision or sound, and there are already many models in research for this. The reasoning part is relatively new, including perhaps the current trend on using embedding and memory techniques. As for the action part, there are at least two potential models &amp;ndash; recurrent neural networks and reinforcement learning. One attempts to model the past, and the other try to predict the future reward.&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s go back to the question of learning to sum. How can we discover whether the machine can do sum? Well, I think the question is probably a wrong one. A better question is probably this: is computing summation a necessity for the evolution of machines?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On April Fool&#39;s: What is Wrong with RNN?</title>
      <link>http://xzh.me/posts/aprilfoolrnn/</link>
      <pubDate>Wed, 01 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://xzh.me/posts/aprilfoolrnn/</guid>
      <description>&lt;p&gt;Google&amp;rsquo;s April fool surprise: reading characters in reverse order (&lt;a href=&#34;https://com.google/&#34;&gt;https://com.google/&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;It happened to be the case that the character order in Crepe (&lt;a href=&#34;https://github.com/zhangxiangxiao/Crepe&#34;&gt;https://github.com/zhangxiangxiao/Crepe&lt;/a&gt;) is also reversed.&lt;/p&gt;

&lt;p&gt;The original thought was that aligning the end of a document to a fixed position (in this case at the beginning) could make it easier for the fully-connected layers to associate meaning with the ending context window.&lt;/p&gt;

&lt;p&gt;This may have the effect of biasing classification towards the end reading of a text, which has a somewhat distant relationship with how recurrent neural network representation can be used for classification, since it decays the influence of document at the beginning but not so much at the end.&lt;/p&gt;

&lt;p&gt;However, later I did (only) one experiment by not reversing the character order for ending alignment, and the classification result did not change in anyway that is statistically significant.&lt;/p&gt;

&lt;p&gt;This was quite though-provoking for me since it might mean that decaying influence like in RNNs is probably a bad thing, or at least useless. Otherwise, why do we need bi-directional RNNs? But then again, if you are using a bi-directional RNN for contextual information, why don&amp;rsquo;t you just use a convolutional network who naturally associates a context for you?&lt;/p&gt;

&lt;p&gt;Then, I went deeper in thought and started to believe that recurrent neural network is sometimes used in a wrong way in the community. Somehow regardless of situation, some researchers take RNNs for granted as the best tool for model sequences (including text, audio, time series, etc). Let me explain why this may not always make sense intuitively.&lt;/p&gt;

&lt;p&gt;What is wrong here is that in most cases the sequence is presented as a whole, but the way RNN models them is by pretending the future values of a sequence is unknown and adapting its internal representation by what is seen so far. Why then, do we need to pretend the future sequence is unknown while in reality it is presented to you as a whole? This apparently does not make sense. The fact that bi-directional RNN gives a performance boost to various sequence modeling tasks is also an evidence for what I am arguing here.&lt;/p&gt;

&lt;p&gt;Now we know the wrong, how do we correct it? Well, the answer is plain and simple: use a convolutional network instead! I even start to believe that convolutional network is the ultimate solution to perception and recognition, as long as the data span a time or space dimension.&lt;/p&gt;

&lt;p&gt;Of course, not all sequences could be presented as whole, especially if the target is to produce actions (such as popping out words for translation, making a robot arm to move, etc). In this case to know both the past and the future may not be possible. Then, if you think even further, recurrent neural network is pretty similar to reinforcement learning &amp;ndash; the difference is that RNNs try to model the past, but RL tries to model the future. This could be the case in which RNN and RL could really shine: produce actions when an artificial agent has already perceived or recognized.&lt;/p&gt;

&lt;p&gt;Putting all these pieces together, I do think in the future state-of-the art A.I. system should use convolutional networks as their perceptive or recognitive part, and a combined form of RNN and RL as the action-producing part. To make this system works well, some memory part might be needed (oh, you want to tell me DeepMind is likely working on this?)&lt;/p&gt;

&lt;p&gt;By the way, today is April Fool&amp;rsquo;s. I am not responsible for whatever I say here. :P&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Landscape of Deep Learning</title>
      <link>http://xzh.me/posts/deeplearninglandscape/</link>
      <pubDate>Tue, 27 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://xzh.me/posts/deeplearninglandscape/</guid>
      <description>

&lt;p&gt;This blog summarizes an answer I posted to a question regarding what kinds of research are there for deep learning, in &lt;a href=&#34;http://www.zhihu.com/question/27608272/answer/37318565&#34;&gt;Zhihu&lt;/a&gt;, a Chinese equivalence of Quora. Surprisingly, that answer drew a lot of attention from many students and young researchers in China and it is currently ranked the second best answer in the subcategory of &amp;ldquo;deep learning&amp;rdquo;. I hope the summarization here could offer my bit of thought to a broader audience by translating that answer to English. Your comments and advices are welcomed!&lt;/p&gt;

&lt;p&gt;In my opinion, the popular deep learning area contains four main research trends. They include optimization, generalization, representation and applications. For each of them, there are both a practical aspect and a theoretical aspect. Let&amp;rsquo;s take a closer look at each of them.&lt;/p&gt;

&lt;h3 id=&#34;optimization:874d3f90a0e473dc1a9e85085a11d03a&#34;&gt;Optimization&lt;/h3&gt;

&lt;p&gt;It seems that almost all problems in deep learning can be reduced to solving mathematical optimization problems. Therefore, numerical optimization is one of the most important research trend for deep learing.&lt;/p&gt;

&lt;p&gt;On the practical side, the most popular method for optimization is still stochastic gradient descent. This extremely simple method seems so stable and robust that many research projects start with it and then end with it. Of course, a lot of tricks could be used to make it faster, such as momentum, psudo-Newton methods and adaptive learning rates. Apart from these, parallelization of optimization for deep models is also a hot topic, with many papers submitted to conferences in both machine learning and distributed systems.&lt;/p&gt;

&lt;p&gt;However, on the theoretical side, the only clear subject we know about is convex optimization. The unfortunate fact that most deep learning models are non-convex poses a huge gap between the practical and theoretical sides in optimization. Motivated by this, some researchers start to dig into the properties of some commonly seen but also narrowly defined models and loss functions, trying to get some information regarding the properties of local optima produced by such non-convex optimization procedures.&lt;/p&gt;

&lt;h3 id=&#34;generalization:874d3f90a0e473dc1a9e85085a11d03a&#34;&gt;Generalization&lt;/h3&gt;

&lt;p&gt;Generalizability concerns whether the training error of a model could approximate the true average testing error on all possible sampling of a given problem&amp;rsquo;s data. We can approximately think of it as the difference between training error and testing error. Before deep learing became popular, one major problem in machine learning has been about controlling the generalization error.&lt;/p&gt;

&lt;p&gt;For practitioners, the generalization problem anticipated by many machine learning researchers simply did not occur. On one hand this is due to the huge scale of datasets we have in this &amp;ldquo;Big Data&amp;rdquo; era. On the other hand, the development of high-performance computing devices such as GPUs enabled us to apply new regularization methods such as Dropout and DropConnect and heavy data augmentation techniques. Whether there are more effective regularization methods is one interesting topic for reseachers. Some ideas include adversarial learning and utlization of extra unlabeled samples.&lt;/p&gt;

&lt;p&gt;For theorists, the effectiveness of deep learning has put heavy doubt on the theories of PAC-learning. All of the theories in this style are about &amp;ldquo;upper-bounds of upperbounds of upper-bounds&amp;hellip;&amp;rdquo;, for which the essence only concerns two kinds of statistical concepts &amp;ndash; concentration inequalities and complexity measurement. Practitioners have shown that these theories have very unrealistic estimation of generalization errors for real-world applications. Personally I believe the current gap betwen practitioners and theorists must be closed. It really should not have been like this, especially when the research in functional analysis is already pretty complete. I also feel that to make advancement in this, we need to better understand the theory in representation of deep learning models, which I will discuss in the following.&lt;/p&gt;

&lt;h3 id=&#34;representation:874d3f90a0e473dc1a9e85085a11d03a&#34;&gt;Representation&lt;/h3&gt;

&lt;p&gt;Representation is a concept recently proposed by researchers in deep learning. For me, representation concerns the relationship between deep learning models and the problems they try to solve. Therefore, the questions about representation are bi-directional: one one hand, we want to know that given a deep learning model what kind of solutions to problems it can represent; on the other hand, we can also ask whether there exists a deep learning model to produce some representation for solving a given problem.&lt;/p&gt;

&lt;p&gt;There are two mainstreams on the practical side. Those who deeply belive in unsupervised learning keep finding better objectives and criteria so that machines can learn to represent by themselves. These include deep belief networks, (restricted) Boltzmann machines, sparse coding and auto-encoders. In the second mainstream, researchers facing practical problems try to design deep learning models to solve them by intuition or inspiration from other disciplines. Some successful examples are convolutional networks for vision and speech, recurrent neural networks that can act upon input, and reinforcement learning models able to play games. Most of the deep learning researchers are in this trend, and it does give them the maximum impact possible.&lt;/p&gt;

&lt;p&gt;However, we really do not have much to say about any theory concerning representation, except for some inspirational ideas borrowed from coginitive psychology or neural science. This is mostly because the theory of representation is drastically different for different problems. The only thing we currently can do now is to think whether and how humans could solve a problem, and then design a model to reduce it to a learning process. This is essentially an immitation game, which is a philosophical definition of intelligence proposed by Alan Turing in his 1950 paper &amp;ldquo;Computing Machinery and Intelligence&amp;rdquo;. Intuitively I also doubt the feasibility of an ultimate representation theory, since it feels like a Laplace&amp;rsquo;s demon whose existence would draw unsolvable paradoxes to the real world.&lt;/p&gt;

&lt;h3 id=&#34;applications:874d3f90a0e473dc1a9e85085a11d03a&#34;&gt;Applications&lt;/h3&gt;

&lt;p&gt;The development of deep learning accompanies its revolutionization of other fields. In the past few years, deep learning has been something like a biased &amp;ldquo;dare or not&amp;rdquo; game &amp;ndash; if you dare to try, there chance of success is most likely guaranteed. Of course, this benefits largely from the explosion on the amount of data, and the availability of more powerful computational resources. At the same time, the knowledge accumulated from exploring deep learning and neural network in the past 30 years also plays an important role.&lt;/p&gt;

&lt;p&gt;In the future, deep learning would continue to solve all kinds of recognition problems including vision (image classification, image segmentation, computational photography), speech (speech recognition), natural language processing (text understanding). At the same, we may see some breakthroughs on machine&amp;rsquo;s ability in acting upon data, like describing an image using text, speech synthesis, automated translation and paragraph summarization. Deep learning may also help us to find approximated solutions to NP-hard problems when inputs are constrained to a limited set. All of these are very hot topics in applications of deep learning, and they can eventually bring huge industrial benefits.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>