<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Xiang Zhang</title>
    <link>http://xzh.me/posts/</link>
    <description>Recent content in Posts on Xiang Zhang</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 21 Apr 2015 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://xzh.me/posts/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>One Precondition for Intelligence</title>
      <link>http://xzh.me/posts/precondition/</link>
      <pubDate>Tue, 21 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://xzh.me/posts/precondition/</guid>
      <description>

&lt;p&gt;Medical study shows that two consciousness could exist in the same body, if the connection between the left and the right brain hemispheres are damaged. Does this medical fact tells us something more about intelligence? My opinion is, it is an evidence for the hypothesis that certain deficiency in low-level communication is a precondition for intelligence. I know that sounds crazy or perhaps hard to understand, but please allow me to explain.&lt;/p&gt;

&lt;h3 id=&#34;a-different-understanding-of-alien-hand-syndrome:45530e5fe76f4db5de7bc9f517c17a55&#34;&gt;A Different Understanding of Alien Hand Syndrome&lt;/h3&gt;

&lt;p&gt;This Friday night I was watching the rather dramatized and thrilling episodes of &amp;ldquo;Dark Matters: Twisted But True&amp;rdquo;. In episode 4 of season 1, there is a story about alien hand syndrome, a rare neurological disorder that causes hand movement without the person being aware of what is happening or having control over the action. In the story, a woman&amp;rsquo;s left hand tried to kill herself without her being able to control it. After some thrilling mood rendering, it was revealed that her brain had some permanent damage resulted from a stroke prior to the syndrome. The problem was that the connection between her brain&amp;rsquo;s left and right hemispheres was broken, and it created two consciousness in the same body.&lt;/p&gt;

&lt;p&gt;This reminds me of a similar illustration in Dr. Michio Kaku&amp;rsquo;s recent book of popular science &amp;ndash; &amp;ldquo;The Future of the Mind: The Scientific Quest to Understand, Enhance, and Empower the Mind&amp;rdquo; &amp;ndash; in whose chapter 1 there is a section named &amp;ldquo;The Split-Brain Paradox&amp;rdquo;. It features the research by Dr. Roger Sperry, a Nobel laureate who studied the effects of split-brain, in particular the fact that two consciousness could exist in the two hemispheres if the connection between them is broken. He also imagined that &amp;ldquo;both the left and the right hemisphere may be conscious simultaneously in different, even in mutually conflicting, mental experiences that run along in parallel.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Following Dr. Roger Sperry&amp;rsquo;s thought, both the TV show episode and Dr. Michio Kaku&amp;rsquo;s book posit on the possibility that there are two consciousness existing in everone&amp;rsquo;s mind at every moment. Dr. Michio Kaku even interviewed Dr. Michael Gazzaniga of the University of California, Santa Barbara, for how experiments can be done to test the theory. The book mentioned a safe way to communicate separately to each hemisphere without the knowledge of the other, and it sucessfully shows that two hemispheres would give different answers to the same question.&lt;/p&gt;

&lt;p&gt;However, there is an apparent logical flaw in the above mentioned thought and the corresponding experiments. This is one very apparent example of causation fallacy, in which there are two possible causal reasoning routines to explain the same results. One is of course, as Dr. Sperry and Dr. Kaku believe, that there were two consciousness exist in the same skull for anyone at any time. The other obvious and often ommitted possibility is that there was only one consciousness in one&amp;rsquo;s brain, until there is a physical disconnection between the two hemispheres.&lt;/p&gt;

&lt;p&gt;If you ask me which theory I believe, I would say the second one. This is because to prove the first theory, one would have to show that when the two hemispheres are in normal connection there exists two consciousness at the same time. This is unlikely to be successful because normal people apparently posit themselves as one whole consciousness with coherent behavior in all body parts. One may wonder then, how about dissociative identity disorder (DID) in which one person could have multiple personalities? Well, the answer is that it does not disqualify our second theory because the disorder does not show multiple personalities at the same time. Rather, multiple dissociated personality states would alternately control a person&amp;rsquo;s behavior.&lt;/p&gt;

&lt;p&gt;One can debate the causation fallacy above endlessly untill some experiments can be done to prove one and disprove the other. But before that, I will assume the second theory is true throughout the rest of the blog.&lt;/p&gt;

&lt;h3 id=&#34;communication-not-as-a-part-of-intelligence:45530e5fe76f4db5de7bc9f517c17a55&#34;&gt;Communication (not?) as a Part of Intelligence&lt;/h3&gt;

&lt;p&gt;The focus of the split-brain phenomenon rests on the connection between left and right hemispheres. If the connection is broken, then one can easily see signs of two consciousness. This means that to be a coherent intelligent entity, the place in which consciousness exists (in this case the brain) must have proper physical connection inside its every component. The necessity of such connection suggests that communication is a crucial part for intelligence.&lt;/p&gt;

&lt;p&gt;However, communication itself is an ambiguous word. For example, communication between humans could mean natural languages, facial expressions or body movements. One usually thinks that this type of communication is different from the type between the two hemispheres of the brain &amp;ndash; which consists of eletronic activity and chemical reactions. If we consider each human as an intelligent entity, then I will call the first kind a high-level communication, and the second kind inside brain as low-level communication. With this dichotomy, I can explain things in a much clearer fashion.&lt;/p&gt;

&lt;p&gt;In the famous paper &amp;ldquo;Computing Machinery and Intelligence&amp;rdquo;, Alan Turing argues that the Turing test could be a philosophical definition for intelligence. The test is essentially deciding whether a machine has intelligence by how good it immitates a human. The decision is made by the judge in a probably and approximately correct way, by trying to distinguish between the machine and the human through questioning and answering. This implicitly indicates that communication (the questioning and answering process) is necessary in deciding whether a machine has intelligence. This is true at least for humans, who unanimously think of each other as an intelligent entity.&lt;/p&gt;

&lt;p&gt;One thing I do not think Turing test encompasses is the possibility of intelligence that does not communicate with humans, and also intelligence that is different from or beyond human comprehension. It only judges whether something is intelligence using one unanimously agreed intelligent entity - human, but I do not believe human is the only possible intelligent entity. It is possible for intelligent machines to communicate with each other in a language that is not English or Chinese, but still could be thought of as being intelligent. However, in philosophy there is probably no better way to define intelligence other than Turing test, because intelligence is a meaningless entity in many people&amp;rsquo;s belief. This belief may even include the definition itself.&lt;/p&gt;

&lt;p&gt;All the above discussions are about high-level communication. I think that people can argue endlessly about whether high-level communication is a necessity for intelligence, and I just gave one example above. On the other hand, the question that whether low-level communication is necessary for intelligence is not an ambiguous question in my opinion, especially for the boundary of something being one or multiple intelligent entity. This is already proved by the alient hand syndrome mentioned in the previous section &amp;ndash; low-level communication is a necessity.&lt;/p&gt;

&lt;h3 id=&#34;one-precondition-for-intelligence:45530e5fe76f4db5de7bc9f517c17a55&#34;&gt;One Precondition for Intelligence&lt;/h3&gt;

&lt;p&gt;Of course, being intelligent alone seems very meaningless. The meaning of being intelligent, at least in the sense for humans, partially rests in the fact that we can communicate with other intelligent entities. On the other hand, if Turing&amp;rsquo;s definition on intelligence is correct, then to decide whether some machine is intelligence at least two other intelligent entities must present &amp;ndash; one to be immitated and the other the judge. Combining with out previous discussion on high-level and low-evel communication, this seemingly boring reasoning gives us a quite striking conclusion: one necessity for the existence of intelligence is that there must be certain deficiency in low-level communication.&lt;/p&gt;

&lt;p&gt;The reason that low-level communication must be deficient for there to be multiple intelligent entity is because if there was low-level communication, the supposedly different intelligent entity would better end up being one intelligence. Speaking in a more abstract way, this makes sense because if low-level communication was efficient, making all the intelligent components a coherent single intelligent entity is simply also more efficient. This analogy is actually being used by software and hardware engineers everyday, such as we never transfer data across the Internet if it was simply a memory copy inside the same computer.&lt;/p&gt;

&lt;p&gt;An acute reader might question at this point that I was not able to define in a clear way what is the difference between low-level and high-level communication. You are right. All that I can say about the difference between low-level and high-level communication is relative to humans. We ought to think natural language as a form of high-level communication, and the electronic and chemical reactions happening in our brain as a form of low-level communication. The reason why we need natural language in the first place is that we want to be more efficient as a whole, under the precondition that low-level communication is deficient.&lt;/p&gt;

&lt;p&gt;Such relativity has more profound implications. It is an open question whether relative to a country, the communication between its citizens is a low-level one whereas the political agenda in between contries is a form of high-level communication, although the former one is by the means of natural languages. If such hypothesis could be modeled in a computational way, this may open another possibility of modeling society by computation. Then, this could probably enable us to use rigorous methamatics to analyze both societies and individuals, by assuming they are relative intelligent entities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dataset Duplication Issues for Text Understanding from Scratch</title>
      <link>http://xzh.me/posts/datasetdup/</link>
      <pubDate>Tue, 07 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://xzh.me/posts/datasetdup/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;Update April 9th 2015: In wake of dataset duplication issues for the Amazon reviews dataset, professor &lt;a href=&#34;http://cseweb.ucsd.edu/~jmcauley&#34;&gt;Julian McAuley&lt;/a&gt; updated their &lt;a href=&#34;http://snap.stanford.edu/data/web-Amazon.html&#34;&gt;SNAP Amazon reviews dataset distribution webpage&lt;/a&gt; with an extra note and an extra data duplication file.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On April 6th 2015, we discovered some issues related to the datasets used in our technical report &lt;a href=&#34;http://arxiv.org/abs/1502.01710&#34;&gt;&amp;ldquo;Text Understanding from Scratch&amp;rdquo;&lt;/a&gt;. These issues include multiple instances of the same sample, and overlaps between training and testing data. These issues were first discovered by Alec Radford, head of research at &lt;a href=&#34;https://indico.io&#34;&gt;indico&lt;/a&gt;, who carefully checked a few of our released datasets and found them. I want offer my thanks to him.&lt;/p&gt;

&lt;p&gt;Here is what I am going to do to solve this issue:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Remove Amazon review and Sogou news datasets from release. The DBPedia dataset is completely clean.&lt;/li&gt;
&lt;li&gt;Put a notification in our technical report&amp;rsquo;s first page, untill we can resolve these issues and obtain new experiment results. This is already submitted to arXiv, but it may take a few days to appear online.&lt;/li&gt;
&lt;li&gt;Solve the issues in the datasets, and redo all related experiments.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I apologize for any inconvenience and inaccurate information resulted from this. I am aware that this technical report drew a lot of attention from fellow researchers all around the world, and I will do my best to resolve the issues as soon as possible.&lt;/p&gt;

&lt;h3 id=&#34;what-are-the-issues:6d4327301e838a9c8bdd4a7eb0a3f227&#34;&gt;What are the issues?&lt;/h3&gt;

&lt;p&gt;As said before, the issues are multiple instances of the same sample, and overlaps between training and testing data. I am still investigating the reasons behind these issues, but so far it looks like there were duplications from the datasets&amp;rsquo; original owners and distributors. Below is a break down of the issues for the datasets in the original technical report.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;DBPedia&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Duplication in Train: 0 of 560000, 0 %&lt;/li&gt;
&lt;li&gt;Duplication in Test: 0 of 70000, 0 %&lt;/li&gt;
&lt;li&gt;Overlap: 0 of 70000, 0%&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yahooo Answers&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Duplication in Train: 434 of 1400000, 0%&lt;/li&gt;
&lt;li&gt;Duplication in Test: 0 of 50000, 0%&lt;/li&gt;
&lt;li&gt;Overlap: 32 of 50000, 0%&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Amazon Review Full Score&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Duplication in Train: 1434980 of 5000000, 29%&lt;/li&gt;
&lt;li&gt;Duplication in Test: 175671 of 1250000, 14%&lt;/li&gt;
&lt;li&gt;Overlap: 513193 of 1250000, 41%&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Amazon Review Polarity&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Duplication in Train: 1854042 of 6000000, 31%&lt;/li&gt;
&lt;li&gt;Duplication in Test: 162916 of 900000, 18%&lt;/li&gt;
&lt;li&gt;Overlap: 352296 of 900000, 39%&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;AGNews Corpus&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Duplication in Train: 13423 of 160000, 8%&lt;/li&gt;
&lt;li&gt;Duplication in Test: 16 of 4400, 0%&lt;/li&gt;
&lt;li&gt;Overlap: 655 of 4400, 15%&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sogou News Corpus&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Duplication in Train: 194783 of 750000, 26%&lt;/li&gt;
&lt;li&gt;Duplication in Test: 6876 of 50000, 14%&lt;/li&gt;
&lt;li&gt;Overlap: 15789 of 50000, 32%&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;is-the-crepe-code-affected:6d4327301e838a9c8bdd4a7eb0a3f227&#34;&gt;Is the Crepe code affected?&lt;/h3&gt;

&lt;p&gt;The release of &lt;a href=&#34;https://github.com/zhangxiangxiao/Crepe&#34;&gt;Crepe&lt;/a&gt; is idependent of the datasets. It is currently not affected by this issue. Also, because the DBPedia dataset is clean, the example usage for Crepe is still totally valid. The DBPedia dataset is kept online.&lt;/p&gt;

&lt;p&gt;That said, if you found bugs or issues with Crepe code, please let me know.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On April Fool&#39;s: What is Wrong with RNN?</title>
      <link>http://xzh.me/posts/aprilfoolrnn/</link>
      <pubDate>Wed, 01 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://xzh.me/posts/aprilfoolrnn/</guid>
      <description>&lt;p&gt;Google&amp;rsquo;s April fool surprise: reading characters in reverse order (&lt;a href=&#34;https://com.google/&#34;&gt;https://com.google/&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;It happened to be the case that the character order in Crepe (&lt;a href=&#34;https://github.com/zhangxiangxiao/Crepe&#34;&gt;https://github.com/zhangxiangxiao/Crepe&lt;/a&gt;) is also reversed.&lt;/p&gt;

&lt;p&gt;The original thought was that aligning the end of a document to a fixed position (in this case at the beginning) could make it easier for the fully-connected layers to associate meaning with the ending context window.&lt;/p&gt;

&lt;p&gt;This may have the effect of biasing classification towards the end reading of a text, which has a somewhat distant relationship with how recurrent neural network representation can be used for classification, since it decays the influence of document at the beginning but not so much at the end.&lt;/p&gt;

&lt;p&gt;However, later I did (only) one experiment by not reversing the character order for ending alignment, and the classification result did not change in anyway that is statistically significant.&lt;/p&gt;

&lt;p&gt;This was quite though-provoking for me since it might mean that decaying influence like in RNNs is probably a bad thing, or at least useless. Otherwise, why do we need bi-directional RNNs? But then again, if you are using a bi-directional RNN for contextual information, why don&amp;rsquo;t you just use a convolutional network who naturally associates a context for you?&lt;/p&gt;

&lt;p&gt;Then, I went deeper in thought and started to believe that recurrent neural network is sometimes used in a wrong way in the community. Somehow regardless of situation, some researchers take RNNs for granted as the best tool for model sequences (including text, audio, time series, etc). Let me explain why this may not always make sense intuitively.&lt;/p&gt;

&lt;p&gt;What is wrong here is that in most cases the sequence is presented as a whole, but the way RNN models them is by pretending the future values of a sequence is unknown and adapting its internal representation by what is seen so far. Why then, do we need to pretend the future sequence is unknown while in reality it is presented to you as a whole? This apparently does not make sense. The fact that bi-directional RNN gives a performance boost to various sequence modeling tasks is also an evidence for what I am arguing here.&lt;/p&gt;

&lt;p&gt;Now we know the wrong, how do we correct it? Well, the answer is plain and simple: use a convolutional network instead! I even start to believe that convolutional network is the ultimate solution to perception and recognition, as long as the data span a time or space dimension.&lt;/p&gt;

&lt;p&gt;Of course, not all sequences could be presented as whole, especially if the target is to produce actions (such as popping out words for translation, making a robot arm to move, etc). In this case to know both the past and the future may not be possible. Then, if you think even further, recurrent neural network is pretty similar to reinforcement learning &amp;ndash; the difference is that RNNs try to model the past, but RL tries to model the future. This could be the case in which RNN and RL could really shine: produce actions when an artificial agent has already perceived or recognized.&lt;/p&gt;

&lt;p&gt;Putting all these pieces together, I do think in the future state-of-the art A.I. system should use convolutional networks as their perceptive or recognitive part, and a combined form of RNN and RL as the action-producing part. To make this system works well, some memory part might be needed (oh, you want to tell me DeepMind is likely working on this?)&lt;/p&gt;

&lt;p&gt;By the way, today is April Fool&amp;rsquo;s. I am not responsible for whatever I say here. :P&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Landscape of Deep Learning</title>
      <link>http://xzh.me/posts/deeplearninglandscape/</link>
      <pubDate>Tue, 27 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://xzh.me/posts/deeplearninglandscape/</guid>
      <description>

&lt;p&gt;This blog summarizes an answer I posted to a question regarding what kinds of research are there for deep learning, in &lt;a href=&#34;http://www.zhihu.com/question/27608272/answer/37318565&#34;&gt;Zhihu&lt;/a&gt;, a Chinese equivalence of Quora. Surprisingly, that answer drew a lot of attention from many students and young researchers in China and it is currently ranked the second best answer in the subcategory of &amp;ldquo;deep learning&amp;rdquo;. I hope the summarization here could offer my bit of thought to a broader audience by translating that answer to English. Your comments and advices are welcomed!&lt;/p&gt;

&lt;p&gt;In my opinion, the popular deep learning area contains four main research trends. They include optimization, generalization, representation and applications. For each of them, there are both a practical aspect and a theoretical aspect. Let&amp;rsquo;s take a closer look at each of them.&lt;/p&gt;

&lt;h3 id=&#34;optimization:874d3f90a0e473dc1a9e85085a11d03a&#34;&gt;Optimization&lt;/h3&gt;

&lt;p&gt;It seems that almost all problems in deep learning can be reduced to solving mathematical optimization problems. Therefore, numerical optimization is one of the most important research trend for deep learing.&lt;/p&gt;

&lt;p&gt;On the practical side, the most popular method for optimization is still stochastic gradient descent. This extremely simple method seems so stable and robust that many research projects start with it and then end with it. Of course, a lot of tricks could be used to make it faster, such as momentum, psudo-Newton methods and adaptive learning rates. Apart from these, parallelization of optimization for deep models is also a hot topic, with many papers submitted to conferences in both machine learning and distributed systems.&lt;/p&gt;

&lt;p&gt;However, on the theoretical side, the only clear subject we know about is convex optimization. The unfortunate fact that most deep learning models are non-convex poses a huge gap between the practical and theoretical sides in optimization. Motivated by this, some researchers start to dig into the properties of some commonly seen but also narrowly defined models and loss functions, trying to get some information regarding the properties of local optima produced by such non-convex optimization procedures.&lt;/p&gt;

&lt;h3 id=&#34;generalization:874d3f90a0e473dc1a9e85085a11d03a&#34;&gt;Generalization&lt;/h3&gt;

&lt;p&gt;Generalizability concerns whether the training error of a model could approximate the true average testing error on all possible sampling of a given problem&amp;rsquo;s data. We can approximately think of it as the difference between training error and testing error. Before deep learing became popular, one major problem in machine learning has been about controlling the generalization error.&lt;/p&gt;

&lt;p&gt;For practitioners, the generalization problem anticipated by many machine learning researchers simply did not occur. On one hand this is due to the huge scale of datasets we have in this &amp;ldquo;Big Data&amp;rdquo; era. On the other hand, the development of high-performance computing devices such as GPUs enabled us to apply new regularization methods such as Dropout and DropConnect and heavy data augmentation techniques. Whether there are more effective regularization methods is one interesting topic for reseachers. Some ideas include adversarial learning and utlization of extra unlabeled samples.&lt;/p&gt;

&lt;p&gt;For theorists, the effectiveness of deep learning has put heavy doubt on the theories of PAC-learning. All of the theories in this style are about &amp;ldquo;upper-bounds of upperbounds of upper-bounds&amp;hellip;&amp;rdquo;, for which the essence only concerns two kinds of statistical concepts &amp;ndash; concentration inequalities and complexity measurement. Practitioners have shown that these theories have very unrealistic estimation of generalization errors for real-world applications. Personally I believe the current gap betwen practitioners and theorists must be closed. It really should not have been like this, especially when the research in functional analysis is already pretty complete. I also feel that to make advancement in this, we need to better understand the theory in representation of deep learning models, which I will discuss in the following.&lt;/p&gt;

&lt;h3 id=&#34;representation:874d3f90a0e473dc1a9e85085a11d03a&#34;&gt;Representation&lt;/h3&gt;

&lt;p&gt;Representation is a concept recently proposed by researchers in deep learning. For me, representation concerns the relationship between deep learning models and the problems they try to solve. Therefore, the questions about representation are bi-directional: one one hand, we want to know that given a deep learning model what kind of solutions to problems it can represent; on the other hand, we can also ask whether there exists a deep learning model to produce some representation for solving a given problem.&lt;/p&gt;

&lt;p&gt;There are two mainstreams on the practical side. Those who deeply belive in unsupervised learning keep finding better objectives and criteria so that machines can learn to represent by themselves. These include deep belief networks, (restricted) Boltzmann machines, sparse coding and auto-encoders. In the second mainstream, researchers facing practical problems try to design deep learning models to solve them by intuition or inspiration from other disciplines. Some successful examples are convolutional networks for vision and speech, recurrent neural networks that can act upon input, and reinforcement learning models able to play games. Most of the deep learning researchers are in this trend, and it does give them the maximum impact possible.&lt;/p&gt;

&lt;p&gt;However, we really do not have much to say about any theory concerning representation, except for some inspirational ideas borrowed from coginitive psychology or neural science. This is mostly because the theory of representation is drastically different for different problems. The only thing we currently can do now is to think whether and how humans could solve a problem, and then design a model to reduce it to a learning process. This is essentially an immitation game, which is a philosophical definition of intelligence proposed by Alan Turing in his 1950 paper &amp;ldquo;Computing Machinery and Intelligence&amp;rdquo;. Intuitively I also doubt the feasibility of an ultimate representation theory, since it feels like a Laplace&amp;rsquo;s demon whose existence would draw unsolvable paradoxes to the real world.&lt;/p&gt;

&lt;h3 id=&#34;applications:874d3f90a0e473dc1a9e85085a11d03a&#34;&gt;Applications&lt;/h3&gt;

&lt;p&gt;The development of deep learning accompanies its revolutionization of other fields. In the past few years, deep learning has been something like a biased &amp;ldquo;dare or not&amp;rdquo; game &amp;ndash; if you dare to try, there chance of success is most likely guaranteed. Of course, this benefits largely from the explosion on the amount of data, and the availability of more powerful computational resources. At the same time, the knowledge accumulated from exploring deep learning and neural network in the past 30 years also plays an important role.&lt;/p&gt;

&lt;p&gt;In the future, deep learning would continue to solve all kinds of recognition problems including vision (image classification, image segmentation, computational photography), speech (speech recognition), natural language processing (text understanding). At the same, we may see some breakthroughs on machine&amp;rsquo;s ability in acting upon data, like describing an image using text, speech synthesis, automated translation and paragraph summarization. Deep learning may also help us to find approximated solutions to NP-hard problems when inputs are constrained to a limited set. All of these are very hot topics in applications of deep learning, and they can eventually bring huge industrial benefits.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>